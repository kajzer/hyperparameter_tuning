{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic hyperparameter tuning for machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating machine learning algorithm introduces us with concept of hyperparameters. Tuning hyperparameters is time consuming and can radically influence performance of the algorithm. Choosing right hyperparameter for your model is crucial task that can make model viable solution to a problem and enable it to generalize on different data. Same model with different set of data may require entirely different hyperparameters that will need to be tuned to a problem since we don’t know them beforehand. Given the above hyperparameter optimization is the problem of optimizing a loss function over a configuration space. \n",
    "\n",
    "## What are hyperparameters?\n",
    "\n",
    "Hyperparameters are not model parameters and the can’t be learned from the data in contrast to model parameters. Model parameters are properties of model that will be adjusted during loss function optimization (weights or bias) while hyperparameters are properties of a model that determine who we conduct training process (learning rate, maximum depth of decision tree). \n",
    "\n",
    "## Methodology od hyperparameter tuning\n",
    "\n",
    "There are several approaches for hyperparameter tuning. One of them is manual tuning which in effect takes away time from other steps of machine learning pipeline like feature engineering. This approach is very inefficient and oftentimes is substituted with grid and random search. Those two are automatic but require long run times since they evaluate areas of parameter grid that don’t give good results. Since manual search is taking up human time and grid and random search is time and computationally inefficient new methods have been proposed. \n",
    "\n",
    "#### Grid Search      \n",
    "\n",
    "Grid search is most basic form of hyperparameter optimization, also called parameter sweep. It trains algorithm for all combinations of a manually specified hyperparameter space at the same time measuring performance with some metric (usually used with K-Fold cross validation). One of the drawbacks to performing grid search are fixed parameters in search space which combinations can omit minimum for specific performance metric.\n",
    "\n",
    "#### Random Search\n",
    "\n",
    "Random search randomly samples search space and evaluates sets from a specified probability space. Limits exhaustive enumeration of all combinations of random search. It can outperform grid search performance by assuming that in most data sets only a few of the hyperparameters matter.\n",
    "\n",
    "#### Bayesian Optimisation\n",
    "\n",
    "Bayesian approach finds extrema of objective function in informed way by keeping track of  previous results. Grid and random search perform every experiment in isolation and every next evaluation is not able to use information from previous runs to improve. Bayesian optimization thanks to record of previous evaluation is able to create a probabilistic model mapping hyperparameters to a probability of a score on the objective function. This model is called surrogate function or response surface. Picking next sample data for evaluation is done by acquisition function. Popular acquisition functions are:\n",
    "\n",
    "Maximum Probability of Improvement \n",
    "- Expected Improvement\n",
    "- Upper Confidence Bound\n",
    "- Expected loss criterion\n",
    "\n",
    "\n",
    "Bayesian optimization belongs to a class of sequential model-based optimization (SMBO) algorithms that allow for one to use the results of our previous iteration to improve our sampling method of the next experiment. Types of SMBO’s differ as well by the choice of surrogate model. Popular surrogate models are:\n",
    "\n",
    "- Gaussian Processes \n",
    "- Random Forest Regressions \n",
    "- Tree Parzen Estimators \n",
    "\n",
    "### Lets create few evaluations of hyperparameter optimization modules and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ IMPORTS ################\n",
    "\n",
    "############ basic imports ##########\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "############ model imports ##########\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "############ metrics ################\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "############ model select sklearn ##\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "############ basic training data ####\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "############ settings #########\n",
    "random_state=123\n",
    "n_iter=50\n",
    "num_folds=2\n",
    "verbose=0\n",
    "\n",
    "############ silient mode ###########\n",
    "import warnings                         \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "########### SEARCH MODULES ##########\n",
    "\n",
    "########### sklearn search methods ##\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "########### hpsklearn search methods #\n",
    "from hpsklearn import HyperoptEstimator, xgboost_regression\n",
    "\n",
    "########### hyperopt search methods###\n",
    "from hyperopt import fmin, tpe, hp, anneal, Trials\n",
    "\n",
    "########### skopt search methods #####\n",
    "from skopt import gp_minimize, forest_minimize, dummy_minimize, gbrt_minimize #gp minimize same as Bayes search\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import BayesSearchCV\n",
    "########### Handling errors while importing BayesSearch #####\n",
    "class BayesSearchCV(BayesSearchCV):\n",
    "    def _run_search(self, x): raise BaseException('Use newer skopt')\n",
    "        \n",
    "########## GPyOpt search methods #####\n",
    "import GPy\n",
    "import GPyOpt\n",
    "\n",
    "########## evolutionary search methods #\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "\n",
    "########## btb search methods #########\n",
    "from btb import HyperParameter, ParamTypes\n",
    "from btb.tuning import GP, Uniform\n",
    "\n",
    "######### tpot imports ################\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "######### auto_ml imports #############\n",
    "from auto_ml import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### implemantation of context manager to get time and collect data from different modules ################\n",
    "class timer:\n",
    "    def __init__(self, name):\n",
    "        self.name          = name\n",
    "        self.cv_score      = None\n",
    "        self.test_score    = None\n",
    "        self.object        = None\n",
    "        self.plot_data     = None\n",
    "        self.best_params   = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self._t0 = time.time()\n",
    "        print(f'[{self.name}] -- Starting tuning')\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *params):\n",
    "        self.time = time.time() - self._t0\n",
    "        print(f'[{self.name}] -- Finished tuning')\n",
    "        if isinstance(self.plot_data, pd.DataFrame):\n",
    "            self.plot_data['score'] = self.plot_data['score'].astype('float')\n",
    "            self.plot_data['learning_rate'] = self.plot_data['learning_rate'].astype('float')\n",
    "            self.plot_data['max_depth'] = self.plot_data['max_depth'].astype('int')\n",
    "            self.plot_data['n_estimators'] = self.plot_data['n_estimators'].astype('int')\n",
    "        return None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'##################### [{self.name}] ##################### \\\n",
    "        \\n \\\n",
    "        \\n   evaluated in:          {self.time:.2f} s\\\n",
    "        \\n   best cros val MSE was: {self.cv_score:.2f}\\\n",
    "        \\n   best test MSE was:     {self.test_score:.2f}\\\n",
    "        \\n   best params:           {self.best_params} \\n\\n'\n",
    "    \n",
    "    def print_time(self):\n",
    "        print(f'[{self.name}] evaluated in {self.time:.2f} s')\n",
    "        \n",
    "    def print_summary(self):\n",
    "        print(self)\n",
    "        self.plot()\n",
    "        \n",
    "    def print_score(self):\n",
    "        print(f'[{self.name}] best test MSE was {self.test_score:.2f}')\n",
    "    \n",
    "    def plot(self):\n",
    "        if isinstance(self.plot_data, pd.DataFrame):\n",
    "\n",
    "            print('\\n-----------Ploting graphs for searched parameters-----------\\n')\n",
    "            plt.rcParams['font.size'] = 12\n",
    "            \n",
    "            fig, axs = plt.subplots(4, 1, figsize = (10, 12))\n",
    "            i = 0\n",
    "            for i, hyper in enumerate(['score', 'learning_rate', 'max_depth', 'n_estimators']):\n",
    "                sns.lineplot(data=self.plot_data[hyper], ax = axs[i])\n",
    "                axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper))\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            fig, axs = plt.subplots(4, 1, figsize = (10, 12))\n",
    "            i = 0\n",
    "            for i, hyper in enumerate(['score', 'learning_rate', 'max_depth', 'n_estimators']):\n",
    "                sns.kdeplot(self.plot_data[hyper], label = hyper, linewidth = 2, ax = axs[i], shade = True)\n",
    "                axs[i].set(xlabel = 'Value', ylabel = '{}'.format(hyper), title = '{} distribution'.format(hyper))\n",
    "                    \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 4, figsize = (24, 6))\n",
    "            i = 0\n",
    "            for i, hyper in enumerate(['score', 'learning_rate', 'max_depth', 'n_estimators']):\n",
    "                sns.regplot(x='index', y=hyper, data = self.plot_data.reset_index(), ax = axs[i])\n",
    "                #sns.lineplot(data=grid_search.plot_data[hyper], ax = axs[i])\n",
    "                axs[i].set(\n",
    "                    xlabel = 'Iteration', \n",
    "                    ylabel = '{}'.format(hyper), \n",
    "                    title = '{} over Search'.format(hyper), \n",
    "                    ylim=(self.plot_data[hyper].min() * 0.9 ,self.plot_data[hyper].max() * 1.1)\n",
    "                )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            print('No plot data')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "n = diabetes.data.shape[0]\n",
    "\n",
    "data = diabetes.data\n",
    "targets = diabetes.target\n",
    "\n",
    "# from sklearn.datasets import load_boston\n",
    "# boston = load_boston()\n",
    "# data = pd.DataFrame(boston.data)\n",
    "# data.columns = boston.feature_names\n",
    "# data['PRICE'] = boston.target\n",
    "# data, targets = data.iloc[:,:-1],data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=random_state\n",
    "                                                   )\n",
    "\n",
    "kf = KFold(n_splits=num_folds, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LightGBM model\n",
    "This model is trained as reference point without specifying any parameters. It is done in two steps. First we wrap context manager object in function and than call function. Context manager object purpose is to collect time and data about trained model, training steps and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_basic_lgbm(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('Basic LGBM') as t:\n",
    "        \n",
    "        # instantiate LGBM\n",
    "        model = LGBMRegressor(random_state=random_state)\n",
    "        \n",
    "        # assign test score to ccontext object\n",
    "        t.cv_score = -cross_val_score(\n",
    "            model, \n",
    "            train_data, \n",
    "            train_targets, \n",
    "            cv=kf, \n",
    "            scoring=\"neg_mean_squared_error\", \n",
    "            n_jobs=1, \n",
    "            verbose=verbose\n",
    "        ).mean()\n",
    "        \n",
    "        # assign best model to context object\n",
    "        t.object = model.fit(train_data, train_targets)\n",
    "        # assign test score\n",
    "        t.test_score = mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.best_params = t.object.get_params()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Basic LGBM] -- Starting tuning\n",
      "[Basic LGBM] -- Finished tuning\n",
      "##################### [Basic LGBM] #####################         \n",
      "         \n",
      "   evaluated in:          0.10 s        \n",
      "   best cros val MSE was: 3831.85        \n",
      "   best test MSE was:     2959.59        \n",
      "   best params:           {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 1} \n",
      "\n",
      "\n",
      "No plot data\n"
     ]
    }
   ],
   "source": [
    "basic_lgbm = evaluate_basic_lgbm()\n",
    "print(basic_lgbm)\n",
    "basic_lgbm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "Evaluating standard sklearn GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gridsearch(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('GridSearch') as t:\n",
    "        \n",
    "        model = LGBMRegressor(random_state=random_state)\n",
    "        \n",
    "        param_grid={'learning_rate': np.logspace(-3, -1, 3),\n",
    "                    'max_depth':  np.linspace(2,20, 5,dtype = int),\n",
    "                    'n_estimators': np.linspace(800,1200, 5, dtype = int),\n",
    "                    'random_state': [random_state]}\n",
    "\n",
    "        gs=GridSearchCV(\n",
    "            model, \n",
    "            param_grid, \n",
    "            scoring='neg_mean_squared_error', \n",
    "            fit_params=None, \n",
    "            n_jobs=-1, \n",
    "            cv=kf, \n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        gs.fit(train_data, train_targets)\n",
    "        t.object = gs\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = -t.object.best_score_\n",
    "        t.best_params = t.object.best_params_\n",
    "        \n",
    "        # get pandas frame of parameter data across iterations of search space\n",
    "        t.plot_data=pd.DataFrame(np.transpose([-t.object.cv_results_['mean_test_score'],\n",
    "                                         t.object.cv_results_['param_learning_rate'].data,\n",
    "                                         t.object.cv_results_['param_max_depth'].data,\n",
    "                                         t.object.cv_results_['param_n_estimators'].data]),\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])     \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "Evaluating sklearn RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomsearch(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('RandomSearch') as t:\n",
    "        \n",
    "        model = LGBMRegressor(random_state=random_state, n_jobs=-1)\n",
    "\n",
    "        param_grid_rand={'learning_rate': np.logspace(-4, 0, 100),\n",
    "                         'max_depth':  randint(2,20),\n",
    "                         'n_estimators': randint(100,2000),\n",
    "                         'random_state': [random_state]}\n",
    "\n",
    "        rs=RandomizedSearchCV(\n",
    "            model, \n",
    "            param_grid_rand, \n",
    "            n_iter = n_iter, \n",
    "            scoring='neg_mean_squared_error', \n",
    "            fit_params=None, \n",
    "            n_jobs=-1, \n",
    "            cv=kf, \n",
    "            verbose=verbose, \n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        rs.fit(train_data, train_targets)\n",
    "        t.object = rs\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = -t.object.best_score_\n",
    "        t.best_params = t.object.best_params_\n",
    "        \n",
    "        t.plot_data=pd.DataFrame(np.transpose([-t.object.cv_results_['mean_test_score'],\n",
    "                                         t.object.cv_results_['param_learning_rate'].data,\n",
    "                                         t.object.cv_results_['param_max_depth'].data,\n",
    "                                         t.object.cv_results_['param_n_estimators'].data]),\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_randomsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt objective function\n",
    "def objective_hyperopt(params, random_state=random_state, cv=kf, X=X_train, y=y_train):\n",
    "    \n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'learning_rate': params['learning_rate']}\n",
    "    \n",
    "    \n",
    "    model = LGBMRegressor(random_state=random_state, **params, n_jobs=-1)\n",
    "    \n",
    "    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperopt_tpe(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf, objective=objective_hyperopt):\n",
    "    with timer('Hyperopt TPE') as t:\n",
    "\n",
    "        # defining search space\n",
    "        space={'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),\n",
    "               'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n",
    "               'learning_rate': hp.loguniform('learning_rate', -4, -1)\n",
    "              }\n",
    "\n",
    "        # trials will log information about every evaluation\n",
    "        trials = Trials()\n",
    "\n",
    "        best=fmin(fn=objective, # function to optimize\n",
    "                  space=space, \n",
    "                  algo=tpe.suggest, # optimization algorithm\n",
    "                  max_evals=n_iter, \n",
    "                  trials=trials,\n",
    "                  rstate=np.random.RandomState(random_state)\n",
    "                 )\n",
    "\n",
    "\n",
    "        model = LGBMRegressor(random_state=random_state, n_estimators=int(best['n_estimators']),\n",
    "                              max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\n",
    "        model.fit(train_data, train_targets)\n",
    "        t.object = model\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = objective(best)\n",
    "        t.best_params = best\n",
    "        \n",
    "        tpe_results=np.array([[x['result']['loss'],\n",
    "                      x['misc']['vals']['learning_rate'][0],\n",
    "                      x['misc']['vals']['max_depth'][0],\n",
    "                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])\n",
    "\n",
    "        t.plot_data=pd.DataFrame(tpe_results,\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "        \n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_hyperopt_tpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperopt_anneal(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf, objective=objective_hyperopt):\n",
    "    with timer('Hyperopt Anneal') as t:\n",
    "        \n",
    "        space={'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),\n",
    "               'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n",
    "               'learning_rate': hp.loguniform('learning_rate', -4, -1)\n",
    "              }\n",
    "\n",
    "       \n",
    "        trials = Trials()\n",
    "\n",
    "        best=fmin(fn=objective_hyperopt, \n",
    "                  space=space, \n",
    "                  algo=anneal.suggest, \n",
    "                  max_evals=n_iter, \n",
    "                  trials=trials, \n",
    "                  rstate=np.random.RandomState(random_state) \n",
    "                 )\n",
    "\n",
    "        # computing the score on the test set\n",
    "        model = LGBMRegressor(random_state=random_state, n_estimators=int(best['n_estimators']),\n",
    "                              max_depth=int(best['max_depth']),learning_rate=best['learning_rate'], n_jobs=-1, verbose=1)\n",
    "        model.fit(train_data,train_targets)\n",
    "        t.object = model\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = objective(best)\n",
    "        t.best_params = best\n",
    "        \n",
    "        tpe_results=np.array([[x['result']['loss'],\n",
    "                      x['misc']['vals']['learning_rate'][0],\n",
    "                      x['misc']['vals']['max_depth'][0],\n",
    "                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])\n",
    "\n",
    "        t.plot_data=pd.DataFrame(tpe_results,\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_hyperopt_anneal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## skopt with sklearn api\n",
    "def evaluate_skopt_bayes(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf, optimizer={'base_estimator':'RF', 'acq_func':'PI', 'acq_optimizer':'auto'}):\n",
    "    with timer('Scikit-Optimize API Bayes') as t:\n",
    "        \n",
    "        model = LGBMRegressor(random_state=random_state, n_jobs=-1)\n",
    "        \n",
    "        param_grid_bayes={'learning_rate': Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "                         'max_depth':  Integer(2,20),\n",
    "                         'n_estimators': Integer(100,2000)\n",
    "                         }\n",
    "\n",
    "        bs=BayesSearchCV(model, param_grid_bayes, n_iter = n_iter, scoring='neg_mean_squared_error', fit_params=None, \n",
    "                        n_jobs=-1, cv=kf, random_state=random_state, optimizer_kwargs=optimizer)\n",
    "\n",
    "        bs.fit(train_data, train_targets)\n",
    "\n",
    "        t.object = bs\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = -t.object.best_score_\n",
    "        t.best_params = t.object.best_params_\n",
    "        \n",
    "        t.plot_data=pd.DataFrame(np.transpose([-np.array(t.object.cv_results_['mean_test_score']),\n",
    "                                         t.object.cv_results_['param_learning_rate'],\n",
    "                                         t.object.cv_results_['param_max_depth'],\n",
    "                                         t.object.cv_results_['param_n_estimators']]),\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_skopt_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########skopt Scikit-Optimize with objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "space  = [Integer(2, 20, name='max_depth'),\n",
    "          Real(10**-4, 10**0, \"log-uniform\", name='learning_rate'),\n",
    "          Integer(100, 2000, name='n_estimators')]\n",
    "\n",
    "\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective_skopt(random_state=random_state, cv=kf, X=X_train, y=y_train,**space):    \n",
    "\n",
    "    model = LGBMRegressor(random_state=random_state, n_jobs=-1)\n",
    "    model.set_params(**space)\n",
    "    \n",
    "    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_skopt_bayes_dummy(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('Scikit-Optimize dummy minimize') as t:\n",
    "\n",
    "        res_gp = dummy_minimize(objective_skopt, space, n_calls=n_iter, random_state=random_state)\n",
    "\n",
    "        # computing the score on the test set\n",
    "        model = LGBMRegressor(random_state=random_state, n_estimators=int(res_gp.x[2]),\n",
    "                              max_depth=int(res_gp.x[0]),learning_rate=res_gp.x[1], n_jobs=-1, verbose=verbose)\n",
    "        model.fit(train_data,train_targets)\n",
    "        t.object = model\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = res_gp.fun\n",
    "        t.best_params = {'learning_rate': res_gp.x[1], 'max_depth': int(res_gp.x[0]), 'n_estimators': int(res_gp.x[2])}\n",
    "        \n",
    "        t.plot_data=pd.DataFrame(np.transpose([np.array(res_gp.func_vals),\n",
    "            np.array(res_gp.x_iters)[:,1], #learning rate\n",
    "            np.array(res_gp.x_iters)[:,2], #n estimators\n",
    "            np.array(res_gp.x_iters)[:,0]]), #max depth\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_skopt_bayes_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_skopt_bayes_forest(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('Scikit-Optimize forest minimize') as t:\n",
    "\n",
    "        res_forest = forest_minimize(objective_skopt, space, n_calls=n_iter, random_state=random_state)\n",
    "\n",
    "        model = LGBMRegressor(random_state=random_state, n_estimators=int(res_forest.x[2]),\n",
    "                      max_depth=int(res_forest.x[0]),learning_rate=res_forest.x[1], n_jobs=-1, verbose=verbose)\n",
    "        model.fit(train_data,train_targets)\n",
    "        t.object = model\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = res_forest.fun\n",
    "        t.best_params = {'learning_rate': res_forest.x[1], 'max_depth': int(res_forest.x[0]), 'n_estimators': int(res_forest.x[2])}\n",
    "        \n",
    "        t.plot_data=pd.DataFrame(np.transpose([np.array(res_forest.func_vals),\n",
    "            np.array(res_forest.x_iters)[:,1], #learning rate\n",
    "            np.array(res_forest.x_iters)[:,2], #n estimators\n",
    "            np.array(res_forest.x_iters)[:,0]]), #max depth\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_skopt_bayes_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_skopt_bayes_gbrt(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('Scikit-Optimize gbrt minimize') as t:\n",
    "\n",
    "        res_gbrt = gbrt_minimize(objective_skopt, space, n_calls=n_iter, random_state=random_state)\n",
    "\n",
    "        model = LGBMRegressor(random_state=random_state, n_estimators=int(res_gbrt.x[2]),\n",
    "                      max_depth=int(res_gbrt.x[0]),learning_rate=res_gbrt.x[1], n_jobs=-1, verbose=verbose)\n",
    "        model.fit(train_data,train_targets)\n",
    "        t.object = model\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = res_gbrt.fun\n",
    "        t.best_params = {'learning_rate': res_gbrt.x[1], 'max_depth': int(res_gbrt.x[0]), 'n_estimators': int(res_gbrt.x[2])}\n",
    "        \n",
    "        t.plot_data=pd.DataFrame(np.transpose([np.array(res_gbrt.func_vals),\n",
    "            np.array(res_gbrt.x_iters)[:,1], #learning rate\n",
    "            np.array(res_gbrt.x_iters)[:,2], #n estimators\n",
    "            np.array(res_gbrt.x_iters)[:,0]]), #max depth\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_skopt_bayes_gbrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(random_state)\n",
    "\n",
    "domain =[{'name': 'max_depth',   'type': 'discrete', 'domain': np.linspace(2,20, dtype=int)},\n",
    "       {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.0001, 1)},\n",
    "       {'name': 'n_estimators',  'type': 'discrete', 'domain': np.linspace(100,2000, dtype=int)}\n",
    "      ]\n",
    "\n",
    "\n",
    "def bayes_lgbm_gpy(x, mdl=None, cv=None):\n",
    "    x = np.atleast_2d(x)\n",
    "    fs = np.zeros((x.shape[0], 1))\n",
    "    for i, params in enumerate(x):\n",
    "        dict_params = dict(zip([el['name'] for el in domain], params))\n",
    "        dict_params['max_depth'] = int(dict_params['max_depth'])\n",
    "        dict_params['n_estimators'] = int(dict_params['n_estimators'])\n",
    "        #print(dict_params)\n",
    "        mdl.set_params(**dict_params)\n",
    "        fs[i] = -cross_val_score(mdl, X_train, y_train, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n",
    "    return fs\n",
    "\n",
    "\n",
    "def evaluate_gpyopt(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('GPyOpt') as t:\n",
    "        opt = GPyOpt.methods.BayesianOptimization(f = partial(bayes_lgbm_gpy, mdl=LGBMRegressor(random_state=random_state, n_jobs=-1), cv=kf),       \n",
    "                                                  domain = domain,         # box-constrains of the problem\n",
    "                                                  model_type= 'GP',\n",
    "                                                  acquisition_type='EI',\n",
    "                                                  num_cores=8,\n",
    "                                                  #acquisition_weight = 0.01\n",
    "                                                 )  \n",
    "        # https://gpyopt.readthedocs.io/en/latest/GPyOpt.methods.html?highlight=acquisition_type\n",
    "        opt.run_optimization(max_iter=n_iter)\n",
    "        \n",
    "        model = LGBMRegressor(random_state=random_state, n_estimators=int(opt.x_opt[2]),\n",
    "                      max_depth=int(opt.x_opt[0]), learning_rate=opt.x_opt[1], n_jobs=-1, verbose=1)\n",
    "        t.object = model\n",
    "        t.object.fit(train_data,train_targets)\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score = opt.fx_opt\n",
    "        t.best_params = {'learning_rate': opt.x_opt[1], 'max_depth': int(opt.x_opt[0]), 'n_estimators': int(opt.x_opt[2])}\n",
    "        \n",
    "        t.plot_data=pd.DataFrame(np.transpose([np.array(opt.Y)[:,0],\n",
    "            np.array(opt.X)[:,1], #learning rate\n",
    "            np.array(opt.X)[:,2], #n estimators\n",
    "            np.array(opt.X)[:,0]]), #max depth\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_gpyopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(random_state)\n",
    "\n",
    "param_grid={'learning_rate': np.logspace(-5, -1, 300),\n",
    "            'max_depth':  np.linspace(2, 20, 20-1, dtype = int),\n",
    "            'n_estimators': np.linspace(100 ,2000, 2000-100, dtype = int),\n",
    "            'random_state': [random_state]}\n",
    "\n",
    "# print(\"Size: \", len(param_grid[\"learning_rate\"])*len(param_grid[\"max_depth\"])*len(param_grid[\"n_estimators\"]))\n",
    "def evaluate_evolutionary(train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('Evolutionary Algorithm Search') as t:\n",
    "        model = LGBMRegressor(random_state=random_state, n_jobs=-1)\n",
    "        evol=EvolutionaryAlgorithmSearchCV(model, \n",
    "                                           param_grid, \n",
    "                                           scoring='neg_mean_squared_error', \n",
    "                                           fit_params=None, \n",
    "                                           n_jobs=1, \n",
    "                                           cv=kf, \n",
    "                                           verbose=verbose, \n",
    "                                           population_size=15,\n",
    "                                           gene_mutation_prob=0.60,\n",
    "                                           gene_crossover_prob=0.5,\n",
    "                                           tournament_size=3,\n",
    "                                           generations_number=10)\n",
    "\n",
    "\n",
    "        evol.fit(train_data, train_targets)\n",
    "        t.object = evol\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))\n",
    "        t.cv_score=-t.object.best_score_\n",
    "        t.best_params=t.object.best_params_\n",
    "        \n",
    "        t.plot_data=pd.DataFrame(np.transpose([-np.array(evol.cv_results_['mean_test_score']),\n",
    "                                         [item['learning_rate'] for item in evol.cv_results_['params']],\n",
    "                                         [item['max_depth'] for item in evol.cv_results_['params']],\n",
    "                                         [item['n_estimators'] for item in evol.cv_results_['params']]]),\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_evolutionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(random_state)\n",
    "tunables = [\n",
    "    ('n_estimators', HyperParameter(ParamTypes.INT, [100, 2000])),\n",
    "    ('max_depth', HyperParameter(ParamTypes.INT, [2, 20])),\n",
    "    ('learning_rate', HyperParameter(ParamTypes.FLOAT_EXP, [0.0001, 1]))\n",
    "]\n",
    "\n",
    "def evaluate_btb(tuner=GP(tunables), train_data=X_train, train_targets=y_train, test_data=X_test, test_targets=y_test, kf=kf):\n",
    "    with timer('BTB') as t:\n",
    "\n",
    "    #def tune_lgbm(tuner, random_state=random_state, cv=kf, X=train_data, y=train_targets):\n",
    "        for i in range(n_iter):\n",
    "            params = tuner.propose()\n",
    "\n",
    "            model = LGBMRegressor(random_state=random_state, \n",
    "                                  n_jobs=-1,\n",
    "                                  n_estimators=params['n_estimators'],\n",
    "                                  max_depth=params['max_depth'],\n",
    "                                  learning_rate=params['learning_rate']\n",
    "                                 )\n",
    "\n",
    "            score = cross_val_score(model, train_data, train_targets, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n",
    "            tuner.add(params, score)\n",
    "            \n",
    "            \n",
    "        model = LGBMRegressor(random_state=random_state, n_estimators=tuner._best_hyperparams['n_estimators'],\n",
    "                      max_depth=tuner._best_hyperparams['max_depth'],learning_rate=tuner._best_hyperparams['learning_rate'], n_jobs=-1, verbose=1)\n",
    "        model.fit(train_data,train_targets)\n",
    "        t.object = model\n",
    "        t.cv_score = -tuner._best_score\n",
    "        t.best_params = tuner._best_hyperparams\n",
    "        t.test_score=mean_squared_error(test_targets, t.object.predict(test_data))    \n",
    "        t.plot_data=pd.DataFrame(np.transpose([-np.array(tuner.y),\n",
    "                                         10**tuner.X[:,2],\n",
    "                                         tuner.X[:,1],\n",
    "                                         tuner.X[:,0]]),\n",
    "                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_array.append(evaluate_btb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GridSearch] -- Starting tuning\n",
      "[GridSearch] -- Finished tuning\n",
      "##################### [GridSearch] #####################         \n",
      "         \n",
      "   evaluated in:          6.67 s        \n",
      "   best cros val MSE was: 3438.89        \n",
      "   best test MSE was:     2523.14        \n",
      "   best params:           {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'random_state': 123} \n",
      "\n",
      "\n",
      "[RandomSearch] -- Starting tuning\n",
      "[RandomSearch] -- Finished tuning\n",
      "##################### [RandomSearch] #####################         \n",
      "         \n",
      "   evaluated in:          4.58 s        \n",
      "   best cros val MSE was: 3413.94        \n",
      "   best test MSE was:     2750.58        \n",
      "   best params:           {'learning_rate': 0.002848035868435802, 'max_depth': 2, 'n_estimators': 1332, 'random_state': 123} \n",
      "\n",
      "\n",
      "[Hyperopt TPE] -- Starting tuning\n",
      "[Hyperopt TPE] -- Finished tuning\n",
      "##################### [Hyperopt TPE] #####################         \n",
      "         \n",
      "   evaluated in:          6.53 s        \n",
      "   best cros val MSE was: 3473.88        \n",
      "   best test MSE was:     2777.77        \n",
      "   best params:           {'learning_rate': 0.01861678436575023, 'max_depth': 13.0, 'n_estimators': 110.0} \n",
      "\n",
      "\n",
      "[Hyperopt Anneal] -- Starting tuning\n",
      "[Hyperopt Anneal] -- Finished tuning\n",
      "##################### [Hyperopt Anneal] #####################         \n",
      "         \n",
      "   evaluated in:          4.42 s        \n",
      "   best cros val MSE was: 3537.09        \n",
      "   best test MSE was:     2694.64        \n",
      "   best params:           {'learning_rate': 0.04124879310445036, 'max_depth': 9.0, 'n_estimators': 111.0} \n",
      "\n",
      "\n",
      "[Scikit-Optimize API Bayes] -- Starting tuning\n",
      "[Scikit-Optimize API Bayes] -- Finished tuning\n",
      "##################### [Scikit-Optimize API Bayes] #####################         \n",
      "         \n",
      "   evaluated in:          30.86 s        \n",
      "   best cros val MSE was: 3450.59        \n",
      "   best test MSE was:     2706.23        \n",
      "   best params:           {'learning_rate': 0.0028285803213750596, 'max_depth': 14, 'n_estimators': 887} \n",
      "\n",
      "\n",
      "[Scikit-Optimize dummy minimize] -- Starting tuning\n",
      "[Scikit-Optimize dummy minimize] -- Finished tuning\n",
      "##################### [Scikit-Optimize dummy minimize] #####################         \n",
      "         \n",
      "   evaluated in:          7.49 s        \n",
      "   best cros val MSE was: 3432.74        \n",
      "   best test MSE was:     2636.70        \n",
      "   best params:           {'learning_rate': 0.0026125144567845037, 'max_depth': 3, 'n_estimators': 1330} \n",
      "\n",
      "\n",
      "[Scikit-Optimize forest minimize] -- Starting tuning\n",
      "[Scikit-Optimize forest minimize] -- Finished tuning\n",
      "##################### [Scikit-Optimize forest minimize] #####################         \n",
      "         \n",
      "   evaluated in:          12.15 s        \n",
      "   best cros val MSE was: 3449.77        \n",
      "   best test MSE was:     2714.06        \n",
      "   best params:           {'learning_rate': 0.003328188196195174, 'max_depth': 5, 'n_estimators': 751} \n",
      "\n",
      "\n",
      "[Scikit-Optimize gbrt minimize] -- Starting tuning\n",
      "[Scikit-Optimize gbrt minimize] -- Finished tuning\n",
      "##################### [Scikit-Optimize gbrt minimize] #####################         \n",
      "         \n",
      "   evaluated in:          9.89 s        \n",
      "   best cros val MSE was: 3445.60        \n",
      "   best test MSE was:     2679.64        \n",
      "   best params:           {'learning_rate': 0.014400565628745865, 'max_depth': 17, 'n_estimators': 181} \n",
      "\n",
      "\n",
      "[GPyOpt] -- Starting tuning\n",
      "[GPyOpt] -- Finished tuning\n",
      "##################### [GPyOpt] #####################         \n",
      "         \n",
      "   evaluated in:          29.36 s        \n",
      "   best cros val MSE was: 4386.43        \n",
      "   best test MSE was:     3506.66        \n",
      "   best params:           {'learning_rate': 0.04557019780525949, 'max_depth': 14, 'n_estimators': 953} \n",
      "\n",
      "\n",
      "[Evolutionary Algorithm Search] -- Starting tuning\n",
      "[Evolutionary Algorithm Search] -- Finished tuning\n",
      "##################### [Evolutionary Algorithm Search] #####################         \n",
      "         \n",
      "   evaluated in:          20.93 s        \n",
      "   best cros val MSE was: 3421.70        \n",
      "   best test MSE was:     2759.09        \n",
      "   best params:           {'learning_rate': 0.001768038178457207, 'max_depth': 3, 'n_estimators': 1529, 'random_state': 123} \n",
      "\n",
      "\n",
      "[BTB] -- Starting tuning\n",
      "[BTB] -- Finished tuning\n",
      "##################### [BTB] #####################         \n",
      "         \n",
      "   evaluated in:          3.89 s        \n",
      "   best cros val MSE was: 3439.90        \n",
      "   best test MSE was:     2674.26        \n",
      "   best params:           {'n_estimators': 526, 'max_depth': 4, 'learning_rate': 0.005750524892206498} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(eval_array):\n",
    "    func = f()\n",
    "    print(func)\n",
    "    eval_array[i] = func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4YAAANhCAYAAAC7MqU3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_df=pd.DataFrame(index=range(n_iter))\n",
    "for elm in eval_array:\n",
    "    scores_df[elm.name] = elm.plot_data['score'].cummin()\n",
    "\n",
    "ax = scores_df.plot(figsize=(15, 15))\n",
    "\n",
    "ax.set_xlabel(\"number_of_iterations\")\n",
    "ax.set_ylabel(\"best_cumulative_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE scored:\n",
      "GridSearch - 2523.141\n",
      "RandomSearch - 2750.581\n",
      "Hyperopt TPE - 2777.769\n",
      "Hyperopt Anneal - 2694.643\n",
      "Scikit-Optimize API Bayes - 2706.230\n",
      "Scikit-Optimize dummy minimize - 2636.697\n",
      "Scikit-Optimize forest minimize - 2714.055\n",
      "Scikit-Optimize gbrt minimize - 2679.641\n",
      "GPyOpt - 3506.660\n",
      "Evolutionary Algorithm Search - 2759.086\n",
      "BTB - 2674.263\n"
     ]
    }
   ],
   "source": [
    "print('Test MSE scored:')\n",
    "for elm in eval_array:\n",
    "    print(\"{} - {:.3f}\".format(elm.name, elm.test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>module</th>\n",
       "      <th>time</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>min_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>best_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GridSearch</td>\n",
       "      <td>6.673160</td>\n",
       "      <td>4029.866516</td>\n",
       "      <td>4596.657380</td>\n",
       "      <td>3438.887311</td>\n",
       "      <td>356.777234</td>\n",
       "      <td>2523.140865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Scikit-Optimize dummy minimize</td>\n",
       "      <td>7.493732</td>\n",
       "      <td>4149.902876</td>\n",
       "      <td>5567.267032</td>\n",
       "      <td>3432.740060</td>\n",
       "      <td>591.714643</td>\n",
       "      <td>2636.696703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BTB</td>\n",
       "      <td>3.892791</td>\n",
       "      <td>4267.949820</td>\n",
       "      <td>5483.298115</td>\n",
       "      <td>3439.895488</td>\n",
       "      <td>610.822699</td>\n",
       "      <td>2674.262659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Scikit-Optimize gbrt minimize</td>\n",
       "      <td>9.886046</td>\n",
       "      <td>3835.557352</td>\n",
       "      <td>5536.792532</td>\n",
       "      <td>3445.595057</td>\n",
       "      <td>598.237049</td>\n",
       "      <td>2679.640941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyperopt Anneal</td>\n",
       "      <td>4.424322</td>\n",
       "      <td>4044.759981</td>\n",
       "      <td>4620.078719</td>\n",
       "      <td>3537.089592</td>\n",
       "      <td>272.548183</td>\n",
       "      <td>2694.643487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scikit-Optimize API Bayes</td>\n",
       "      <td>30.856599</td>\n",
       "      <td>3609.819929</td>\n",
       "      <td>5341.096304</td>\n",
       "      <td>3450.590099</td>\n",
       "      <td>408.356173</td>\n",
       "      <td>2706.229847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Scikit-Optimize forest minimize</td>\n",
       "      <td>12.147161</td>\n",
       "      <td>3761.765738</td>\n",
       "      <td>5536.792532</td>\n",
       "      <td>3449.772499</td>\n",
       "      <td>539.017159</td>\n",
       "      <td>2714.055111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>4.584404</td>\n",
       "      <td>4259.892297</td>\n",
       "      <td>5701.840470</td>\n",
       "      <td>3413.942217</td>\n",
       "      <td>607.425657</td>\n",
       "      <td>2750.581122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Evolutionary Algorithm Search</td>\n",
       "      <td>20.931032</td>\n",
       "      <td>3468.553817</td>\n",
       "      <td>4320.988796</td>\n",
       "      <td>3421.702061</td>\n",
       "      <td>124.700322</td>\n",
       "      <td>2759.086266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyperopt TPE</td>\n",
       "      <td>6.531447</td>\n",
       "      <td>4184.970193</td>\n",
       "      <td>4631.380250</td>\n",
       "      <td>3473.879946</td>\n",
       "      <td>360.459850</td>\n",
       "      <td>2777.768522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GPyOpt</td>\n",
       "      <td>29.359994</td>\n",
       "      <td>4784.159838</td>\n",
       "      <td>5785.903613</td>\n",
       "      <td>4386.428347</td>\n",
       "      <td>344.723024</td>\n",
       "      <td>3506.659816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             module       time   mean_score    max_score  \\\n",
       "0                        GridSearch   6.673160  4029.866516  4596.657380   \n",
       "5    Scikit-Optimize dummy minimize   7.493732  4149.902876  5567.267032   \n",
       "10                              BTB   3.892791  4267.949820  5483.298115   \n",
       "7     Scikit-Optimize gbrt minimize   9.886046  3835.557352  5536.792532   \n",
       "3                   Hyperopt Anneal   4.424322  4044.759981  4620.078719   \n",
       "4         Scikit-Optimize API Bayes  30.856599  3609.819929  5341.096304   \n",
       "6   Scikit-Optimize forest minimize  12.147161  3761.765738  5536.792532   \n",
       "1                      RandomSearch   4.584404  4259.892297  5701.840470   \n",
       "9     Evolutionary Algorithm Search  20.931032  3468.553817  4320.988796   \n",
       "2                      Hyperopt TPE   6.531447  4184.970193  4631.380250   \n",
       "8                            GPyOpt  29.359994  4784.159838  5785.903613   \n",
       "\n",
       "      min_score   std_score  best_test_score  \n",
       "0   3438.887311  356.777234      2523.140865  \n",
       "5   3432.740060  591.714643      2636.696703  \n",
       "10  3439.895488  610.822699      2674.262659  \n",
       "7   3445.595057  598.237049      2679.640941  \n",
       "3   3537.089592  272.548183      2694.643487  \n",
       "4   3450.590099  408.356173      2706.229847  \n",
       "6   3449.772499  539.017159      2714.055111  \n",
       "1   3413.942217  607.425657      2750.581122  \n",
       "9   3421.702061  124.700322      2759.086266  \n",
       "2   3473.879946  360.459850      2777.768522  \n",
       "8   4386.428347  344.723024      3506.659816  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_frame = pd.DataFrame(columns=['module', 'time', 'mean_score', 'max_score', 'min_score', 'std_score', 'best_test_score'])\n",
    "stat_frame = stat_frame.append(\n",
    "    pd.DataFrame(\n",
    "        [[elm.name, \n",
    "          elm.time, \n",
    "          elm.plot_data['score'].mean(), \n",
    "          elm.plot_data['score'].max(), \n",
    "          elm.plot_data['score'].min(), \n",
    "          elm.plot_data['score'].std(),\n",
    "          elm.test_score] for elm in eval_array], \n",
    "        columns=['module', 'time', 'mean_score', 'max_score', 'min_score', 'std_score', 'best_test_score']\n",
    "    )\n",
    ")\n",
    "stat_frame.sort_values('best_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# estim = HyperoptEstimator(regressor=xgboost_regression('my_gb'), max_evals=n_iter, trial_timeout=60, seed=random_state)\n",
    "\n",
    "# estim.fit(train_data, train_targets)\n",
    "\n",
    "# print(mean_squared_error(test_targets, estim.predict(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_config = {\n",
    "    'lightgbm.sklearn.LGBMRegressor': {\n",
    "        'learning_rate': np.logspace(-4, -1, 80),\n",
    "        'max_depth':  np.linspace(2,20,18,dtype = int),\n",
    "        'n_estimators': np.linspace(100,2000,1900, dtype = int),\n",
    "        'random_state': [random_state]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -17.47902699438331\n",
      "Generation 2 - Current best internal CV score: -17.47902699438331\n",
      "Generation 3 - Current best internal CV score: -17.47902699438331\n",
      "Generation 4 - Current best internal CV score: -17.47902699438331\n",
      "Generation 5 - Current best internal CV score: -17.47902699438331\n",
      "Generation 6 - Current best internal CV score: -17.47902699438331\n",
      "Generation 7 - Current best internal CV score: -17.47902699438331\n",
      "Generation 8 - Current best internal CV score: -17.47902699438331\n",
      "Generation 9 - Current best internal CV score: -17.47902699438331\n",
      "Generation 10 - Current best internal CV score: -17.47902699438331\n",
      "\n",
      "Best pipeline: LGBMRegressor(input_matrix, learning_rate=0.014606863203649888, max_depth=13, n_estimators=469, random_state=42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTRegressor(config_dict={'lightgbm.sklearn.LGBMRegressor': {'learning_rate': array([0.0001 , 0.00011, ..., 0.09163, 0.1    ]), 'max_depth': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       20]), 'n_estimators': array([ 100,  101, ..., 1998, 2000]), 'random_state': [42]}},\n",
       "       crossover_rate=0.1,\n",
       "       cv=KFold(n_splits=2, random_state=42, shuffle=False),\n",
       "       disable_update_check=False, early_stop=None, generations=10,\n",
       "       max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "       mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n",
       "       periodic_checkpoint_folder=None, population_size=50,\n",
       "       random_state=42, scoring='neg_mean_squared_error', subsample=1.0,\n",
       "       use_dask=False, verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTRegressor(\n",
    "    generations=10, \n",
    "    population_size=50, \n",
    "    verbosity=2,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    random_state=random_state,\n",
    "    #periodic_checkpoint_folder='tpot_results',\n",
    "    config_dict=tpot_config\n",
    ")\n",
    "tpot.fit(train_data, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.36163314071664"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tpot.score(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.36163314071664"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(test_targets, tpot.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lgbmregressor',\n",
       "  LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "         learning_rate=0.014606863203649888, max_depth=13,\n",
       "         min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "         n_estimators=469, n_jobs=-1, num_leaves=31, objective=None,\n",
       "         random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "         subsample=1.0, subsample_for_bin=200000, subsample_freq=1))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fitted_pipeline_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tpot.export('tpot_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### auto_ml##########\n",
    "# from auto_ml.utils import get_boston_dataset\n",
    "\n",
    "# df_train, df_test = get_boston_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(random_state)\n",
    "# Load data\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_train.columns = diabetes.feature_names\n",
    "df_train['targets'] = train_targets\n",
    "df_test = pd.DataFrame(test_data)\n",
    "df_test.columns = diabetes.feature_names\n",
    "df_test['targets'] = test_targets\n",
    "\n",
    "df_train, fl_data = train_test_split(df_train, test_size=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.10\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'epochs': 1000, 'batch_size': 50, 'verbose': 2}\n",
      "Running basic data cleaning\n",
      "Performing feature scaling\n",
      "Fitting DataFrameVectorizer\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model DeepLearningRegressor to predict MEDV\n",
      "Started at:\n",
      "2019-02-10 23:14:42\n",
      "\n",
      "We will stop training early if we have not seen an improvement in validation accuracy in 25 epochs\n",
      "To measure validation accuracy, we will split off a random 10 percent of your training data set\n",
      "Train on 154 samples, validate on 28 samples\n",
      "Epoch 1/1000\n",
      " - 1s - loss: 646.4211 - mean_absolute_error: 23.5612 - mean_absolute_percentage_error: 99.9912 - val_loss: 433.8459 - val_mean_absolute_error: 19.8675 - val_mean_absolute_percentage_error: 99.9548\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 646.0476 - mean_absolute_error: 23.5532 - mean_absolute_percentage_error: 99.9510 - val_loss: 433.4662 - val_mean_absolute_error: 19.8579 - val_mean_absolute_percentage_error: 99.8973\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 645.5453 - mean_absolute_error: 23.5426 - mean_absolute_percentage_error: 99.8990 - val_loss: 432.8853 - val_mean_absolute_error: 19.8433 - val_mean_absolute_percentage_error: 99.8093\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 644.7374 - mean_absolute_error: 23.5255 - mean_absolute_percentage_error: 99.8128 - val_loss: 431.8360 - val_mean_absolute_error: 19.8169 - val_mean_absolute_percentage_error: 99.6503\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 643.2497 - mean_absolute_error: 23.4936 - mean_absolute_percentage_error: 99.6519 - val_loss: 429.7778 - val_mean_absolute_error: 19.7649 - val_mean_absolute_percentage_error: 99.3382\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 640.2596 - mean_absolute_error: 23.4302 - mean_absolute_percentage_error: 99.3337 - val_loss: 425.6613 - val_mean_absolute_error: 19.6608 - val_mean_absolute_percentage_error: 98.7142\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 634.2444 - mean_absolute_error: 23.3017 - mean_absolute_percentage_error: 98.6911 - val_loss: 417.3421 - val_mean_absolute_error: 19.4493 - val_mean_absolute_percentage_error: 97.4568\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 621.1710 - mean_absolute_error: 23.0300 - mean_absolute_percentage_error: 97.3832 - val_loss: 400.0321 - val_mean_absolute_error: 19.0050 - val_mean_absolute_percentage_error: 94.8525\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 592.3736 - mean_absolute_error: 22.4215 - mean_absolute_percentage_error: 94.5101 - val_loss: 361.2691 - val_mean_absolute_error: 17.9787 - val_mean_absolute_percentage_error: 89.1140\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 516.5278 - mean_absolute_error: 20.7984 - mean_absolute_percentage_error: 87.3712 - val_loss: 286.5443 - val_mean_absolute_error: 15.5514 - val_mean_absolute_percentage_error: 76.1600\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 376.8803 - mean_absolute_error: 16.8750 - mean_absolute_percentage_error: 71.1245 - val_loss: 209.8948 - val_mean_absolute_error: 12.7004 - val_mean_absolute_percentage_error: 59.6687\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 265.9415 - mean_absolute_error: 13.4009 - mean_absolute_percentage_error: 54.6646 - val_loss: 152.8491 - val_mean_absolute_error: 10.7205 - val_mean_absolute_percentage_error: 51.5685\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 204.6504 - mean_absolute_error: 11.1746 - mean_absolute_percentage_error: 44.4955 - val_loss: 98.1718 - val_mean_absolute_error: 8.1214 - val_mean_absolute_percentage_error: 45.5060\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 133.7607 - mean_absolute_error: 8.1931 - mean_absolute_percentage_error: 33.1858 - val_loss: 64.5568 - val_mean_absolute_error: 6.4924 - val_mean_absolute_percentage_error: 45.0663\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 97.8563 - mean_absolute_error: 7.1369 - mean_absolute_percentage_error: 33.1866 - val_loss: 69.3841 - val_mean_absolute_error: 6.8276 - val_mean_absolute_percentage_error: 55.0954\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 90.5591 - mean_absolute_error: 6.9643 - mean_absolute_percentage_error: 35.4361 - val_loss: 48.2164 - val_mean_absolute_error: 5.7433 - val_mean_absolute_percentage_error: 40.7009\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 107.6506 - mean_absolute_error: 7.5069 - mean_absolute_percentage_error: 33.6457 - val_loss: 61.7065 - val_mean_absolute_error: 6.3045 - val_mean_absolute_percentage_error: 52.6968\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 85.8911 - mean_absolute_error: 6.8421 - mean_absolute_percentage_error: 36.0878 - val_loss: 65.9186 - val_mean_absolute_error: 6.7235 - val_mean_absolute_percentage_error: 52.3290\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 80.4965 - mean_absolute_error: 6.4297 - mean_absolute_percentage_error: 31.9519 - val_loss: 66.9587 - val_mean_absolute_error: 6.7594 - val_mean_absolute_percentage_error: 51.8059\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 80.5114 - mean_absolute_error: 6.5611 - mean_absolute_percentage_error: 33.6835 - val_loss: 43.1053 - val_mean_absolute_error: 5.1798 - val_mean_absolute_percentage_error: 42.7560\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 85.8757 - mean_absolute_error: 6.6821 - mean_absolute_percentage_error: 33.6520 - val_loss: 38.3255 - val_mean_absolute_error: 4.8072 - val_mean_absolute_percentage_error: 35.4232\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 83.6040 - mean_absolute_error: 6.3769 - mean_absolute_percentage_error: 28.4466 - val_loss: 43.4558 - val_mean_absolute_error: 5.3859 - val_mean_absolute_percentage_error: 39.2810\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 73.4288 - mean_absolute_error: 5.9347 - mean_absolute_percentage_error: 27.3150 - val_loss: 69.3318 - val_mean_absolute_error: 6.9642 - val_mean_absolute_percentage_error: 54.2039\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 72.9410 - mean_absolute_error: 6.6390 - mean_absolute_percentage_error: 33.4965 - val_loss: 31.5479 - val_mean_absolute_error: 4.2438 - val_mean_absolute_percentage_error: 31.6669\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 69.4130 - mean_absolute_error: 5.5947 - mean_absolute_percentage_error: 24.1904 - val_loss: 71.0308 - val_mean_absolute_error: 6.6005 - val_mean_absolute_percentage_error: 46.2991\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 66.1866 - mean_absolute_error: 5.8337 - mean_absolute_percentage_error: 26.8870 - val_loss: 30.8575 - val_mean_absolute_error: 4.3042 - val_mean_absolute_percentage_error: 33.2646\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 68.4106 - mean_absolute_error: 6.0385 - mean_absolute_percentage_error: 28.3498 - val_loss: 67.6954 - val_mean_absolute_error: 7.0154 - val_mean_absolute_percentage_error: 52.9008\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 63.2793 - mean_absolute_error: 6.1050 - mean_absolute_percentage_error: 29.8213 - val_loss: 36.5614 - val_mean_absolute_error: 4.7593 - val_mean_absolute_percentage_error: 34.4011\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 55.5829 - mean_absolute_error: 5.3737 - mean_absolute_percentage_error: 25.2480 - val_loss: 28.8273 - val_mean_absolute_error: 4.0226 - val_mean_absolute_percentage_error: 29.1398\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 55.4462 - mean_absolute_error: 5.4281 - mean_absolute_percentage_error: 24.2864 - val_loss: 28.1700 - val_mean_absolute_error: 4.1088 - val_mean_absolute_percentage_error: 31.6298\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 58.3873 - mean_absolute_error: 5.7265 - mean_absolute_percentage_error: 26.4964 - val_loss: 28.8874 - val_mean_absolute_error: 4.1093 - val_mean_absolute_percentage_error: 30.3108\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 51.1712 - mean_absolute_error: 5.3054 - mean_absolute_percentage_error: 23.8272 - val_loss: 70.3651 - val_mean_absolute_error: 6.3008 - val_mean_absolute_percentage_error: 39.9856\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 55.7908 - mean_absolute_error: 5.4366 - mean_absolute_percentage_error: 24.1289 - val_loss: 41.4265 - val_mean_absolute_error: 5.1124 - val_mean_absolute_percentage_error: 35.3478\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 45.8951 - mean_absolute_error: 4.8240 - mean_absolute_percentage_error: 21.3899 - val_loss: 25.9737 - val_mean_absolute_error: 3.9535 - val_mean_absolute_percentage_error: 29.4762\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 46.2880 - mean_absolute_error: 4.9818 - mean_absolute_percentage_error: 22.3307 - val_loss: 39.6729 - val_mean_absolute_error: 5.2481 - val_mean_absolute_percentage_error: 35.1384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      " - 0s - loss: 46.5460 - mean_absolute_error: 4.9829 - mean_absolute_percentage_error: 21.7383 - val_loss: 21.0951 - val_mean_absolute_error: 3.1907 - val_mean_absolute_percentage_error: 17.0290\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 55.4806 - mean_absolute_error: 5.3400 - mean_absolute_percentage_error: 22.4045 - val_loss: 29.8819 - val_mean_absolute_error: 4.1483 - val_mean_absolute_percentage_error: 27.9002\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 50.2483 - mean_absolute_error: 5.6504 - mean_absolute_percentage_error: 27.1670 - val_loss: 26.9150 - val_mean_absolute_error: 4.1264 - val_mean_absolute_percentage_error: 27.4680\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 38.9047 - mean_absolute_error: 4.6646 - mean_absolute_percentage_error: 20.9697 - val_loss: 18.9116 - val_mean_absolute_error: 3.0994 - val_mean_absolute_percentage_error: 20.0490\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 46.6716 - mean_absolute_error: 4.6483 - mean_absolute_percentage_error: 19.5229 - val_loss: 15.6119 - val_mean_absolute_error: 2.7520 - val_mean_absolute_percentage_error: 18.1489\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 51.1576 - mean_absolute_error: 4.9321 - mean_absolute_percentage_error: 19.8579 - val_loss: 40.6932 - val_mean_absolute_error: 4.8133 - val_mean_absolute_percentage_error: 31.6666\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 45.1369 - mean_absolute_error: 5.0401 - mean_absolute_percentage_error: 22.6499 - val_loss: 47.1230 - val_mean_absolute_error: 5.5549 - val_mean_absolute_percentage_error: 37.7196\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 42.7124 - mean_absolute_error: 5.1072 - mean_absolute_percentage_error: 23.7093 - val_loss: 27.8384 - val_mean_absolute_error: 3.7034 - val_mean_absolute_percentage_error: 21.3862\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 41.1617 - mean_absolute_error: 4.6119 - mean_absolute_percentage_error: 20.7014 - val_loss: 31.2319 - val_mean_absolute_error: 4.1994 - val_mean_absolute_percentage_error: 22.4961\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 39.5783 - mean_absolute_error: 4.6369 - mean_absolute_percentage_error: 20.1475 - val_loss: 19.9590 - val_mean_absolute_error: 3.3964 - val_mean_absolute_percentage_error: 25.1651\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 40.7731 - mean_absolute_error: 4.6652 - mean_absolute_percentage_error: 20.3494 - val_loss: 26.4146 - val_mean_absolute_error: 3.7825 - val_mean_absolute_percentage_error: 21.2904\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 36.5748 - mean_absolute_error: 4.4185 - mean_absolute_percentage_error: 20.3083 - val_loss: 18.7072 - val_mean_absolute_error: 3.2762 - val_mean_absolute_percentage_error: 23.8657\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 39.0148 - mean_absolute_error: 4.5462 - mean_absolute_percentage_error: 20.4062 - val_loss: 74.1976 - val_mean_absolute_error: 7.6628 - val_mean_absolute_percentage_error: 49.3445\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 46.5283 - mean_absolute_error: 5.4166 - mean_absolute_percentage_error: 26.3322 - val_loss: 25.5158 - val_mean_absolute_error: 3.6784 - val_mean_absolute_percentage_error: 22.2962\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 39.0791 - mean_absolute_error: 4.6420 - mean_absolute_percentage_error: 20.5805 - val_loss: 18.8361 - val_mean_absolute_error: 3.3007 - val_mean_absolute_percentage_error: 23.8122\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 40.5173 - mean_absolute_error: 4.7441 - mean_absolute_percentage_error: 21.0268 - val_loss: 15.7133 - val_mean_absolute_error: 2.8152 - val_mean_absolute_percentage_error: 17.2284\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 51.5326 - mean_absolute_error: 5.2177 - mean_absolute_percentage_error: 22.8567 - val_loss: 23.7778 - val_mean_absolute_error: 3.8771 - val_mean_absolute_percentage_error: 20.9375\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 37.9074 - mean_absolute_error: 4.3805 - mean_absolute_percentage_error: 19.4947 - val_loss: 24.4726 - val_mean_absolute_error: 3.9459 - val_mean_absolute_percentage_error: 28.2620\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 33.4926 - mean_absolute_error: 4.3110 - mean_absolute_percentage_error: 19.6946 - val_loss: 21.0707 - val_mean_absolute_error: 3.6015 - val_mean_absolute_percentage_error: 26.1405\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 42.5134 - mean_absolute_error: 4.8045 - mean_absolute_percentage_error: 20.7994 - val_loss: 32.7809 - val_mean_absolute_error: 4.2556 - val_mean_absolute_percentage_error: 25.8316\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 53.9446 - mean_absolute_error: 5.2978 - mean_absolute_percentage_error: 22.9819 - val_loss: 25.3191 - val_mean_absolute_error: 3.8436 - val_mean_absolute_percentage_error: 22.6275\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 36.1949 - mean_absolute_error: 4.4645 - mean_absolute_percentage_error: 20.7172 - val_loss: 32.5773 - val_mean_absolute_error: 4.2755 - val_mean_absolute_percentage_error: 26.9466\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 35.3293 - mean_absolute_error: 4.3123 - mean_absolute_percentage_error: 19.6193 - val_loss: 42.3422 - val_mean_absolute_error: 5.5512 - val_mean_absolute_percentage_error: 40.1758\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 45.1268 - mean_absolute_error: 5.1851 - mean_absolute_percentage_error: 24.5244 - val_loss: 41.6218 - val_mean_absolute_error: 5.5977 - val_mean_absolute_percentage_error: 36.5832\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 40.5604 - mean_absolute_error: 4.9418 - mean_absolute_percentage_error: 23.3816 - val_loss: 22.1572 - val_mean_absolute_error: 3.6603 - val_mean_absolute_percentage_error: 24.2817\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 38.4910 - mean_absolute_error: 4.6230 - mean_absolute_percentage_error: 20.5683 - val_loss: 28.5145 - val_mean_absolute_error: 4.1513 - val_mean_absolute_percentage_error: 27.7234\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 36.4202 - mean_absolute_error: 4.5189 - mean_absolute_percentage_error: 20.1309 - val_loss: 15.6299 - val_mean_absolute_error: 2.9894 - val_mean_absolute_percentage_error: 20.4803\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 36.7614 - mean_absolute_error: 4.2746 - mean_absolute_percentage_error: 19.0393 - val_loss: 18.8216 - val_mean_absolute_error: 3.3724 - val_mean_absolute_percentage_error: 24.1686\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 35.3648 - mean_absolute_error: 4.4800 - mean_absolute_percentage_error: 20.3683 - val_loss: 14.1275 - val_mean_absolute_error: 2.6752 - val_mean_absolute_percentage_error: 18.3385\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 36.7630 - mean_absolute_error: 4.2695 - mean_absolute_percentage_error: 19.0714 - val_loss: 14.9745 - val_mean_absolute_error: 2.7784 - val_mean_absolute_percentage_error: 20.2663\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 35.9370 - mean_absolute_error: 4.2591 - mean_absolute_percentage_error: 18.4908 - val_loss: 40.2864 - val_mean_absolute_error: 4.5717 - val_mean_absolute_percentage_error: 27.7578\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 44.9313 - mean_absolute_error: 4.7493 - mean_absolute_percentage_error: 20.1401 - val_loss: 17.7665 - val_mean_absolute_error: 3.3563 - val_mean_absolute_percentage_error: 20.7793\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 33.5229 - mean_absolute_error: 4.0211 - mean_absolute_percentage_error: 18.1358 - val_loss: 75.0496 - val_mean_absolute_error: 6.5454 - val_mean_absolute_percentage_error: 41.7424\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 44.5843 - mean_absolute_error: 5.0594 - mean_absolute_percentage_error: 23.4101 - val_loss: 16.4822 - val_mean_absolute_error: 3.1024 - val_mean_absolute_percentage_error: 21.5484\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 38.9132 - mean_absolute_error: 4.6019 - mean_absolute_percentage_error: 21.1876 - val_loss: 14.0270 - val_mean_absolute_error: 2.6604 - val_mean_absolute_percentage_error: 18.0832\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 29.6660 - mean_absolute_error: 3.8240 - mean_absolute_percentage_error: 17.0649 - val_loss: 16.3912 - val_mean_absolute_error: 3.0770 - val_mean_absolute_percentage_error: 21.6470\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 31.1191 - mean_absolute_error: 4.0623 - mean_absolute_percentage_error: 18.4195 - val_loss: 14.6387 - val_mean_absolute_error: 2.8243 - val_mean_absolute_percentage_error: 18.3105\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 38.0563 - mean_absolute_error: 4.4394 - mean_absolute_percentage_error: 19.6855 - val_loss: 50.9002 - val_mean_absolute_error: 6.1131 - val_mean_absolute_percentage_error: 41.4366\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 35.1486 - mean_absolute_error: 4.7704 - mean_absolute_percentage_error: 23.0137 - val_loss: 31.5483 - val_mean_absolute_error: 4.2249 - val_mean_absolute_percentage_error: 25.5155\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 34.7149 - mean_absolute_error: 4.3845 - mean_absolute_percentage_error: 20.2604 - val_loss: 20.9997 - val_mean_absolute_error: 3.6013 - val_mean_absolute_percentage_error: 25.8242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/1000\n",
      " - 0s - loss: 35.1820 - mean_absolute_error: 4.4453 - mean_absolute_percentage_error: 20.1671 - val_loss: 60.5255 - val_mean_absolute_error: 5.3423 - val_mean_absolute_percentage_error: 30.1489\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 40.9572 - mean_absolute_error: 4.5569 - mean_absolute_percentage_error: 20.2227 - val_loss: 15.5770 - val_mean_absolute_error: 2.9430 - val_mean_absolute_percentage_error: 20.4769\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 32.7447 - mean_absolute_error: 4.1088 - mean_absolute_percentage_error: 18.4575 - val_loss: 15.8169 - val_mean_absolute_error: 3.0716 - val_mean_absolute_percentage_error: 19.4494\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 31.3672 - mean_absolute_error: 4.0178 - mean_absolute_percentage_error: 17.9192 - val_loss: 38.0438 - val_mean_absolute_error: 4.6126 - val_mean_absolute_percentage_error: 27.7178\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 40.5548 - mean_absolute_error: 4.6322 - mean_absolute_percentage_error: 21.2296 - val_loss: 36.7243 - val_mean_absolute_error: 4.6239 - val_mean_absolute_percentage_error: 28.7770\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 37.6769 - mean_absolute_error: 4.6744 - mean_absolute_percentage_error: 21.5830 - val_loss: 35.9866 - val_mean_absolute_error: 4.6909 - val_mean_absolute_percentage_error: 29.9575\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 36.8287 - mean_absolute_error: 4.4105 - mean_absolute_percentage_error: 19.4809 - val_loss: 51.3598 - val_mean_absolute_error: 5.4747 - val_mean_absolute_percentage_error: 34.3560\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 34.8033 - mean_absolute_error: 4.3539 - mean_absolute_percentage_error: 20.3039 - val_loss: 18.2317 - val_mean_absolute_error: 3.2809 - val_mean_absolute_percentage_error: 23.6576\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 34.3089 - mean_absolute_error: 4.3302 - mean_absolute_percentage_error: 19.2605 - val_loss: 16.6990 - val_mean_absolute_error: 3.0821 - val_mean_absolute_percentage_error: 22.3187\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 31.6792 - mean_absolute_error: 4.0976 - mean_absolute_percentage_error: 18.6250 - val_loss: 23.7227 - val_mean_absolute_error: 3.8325 - val_mean_absolute_percentage_error: 22.1518\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 34.4278 - mean_absolute_error: 4.1582 - mean_absolute_percentage_error: 18.5450 - val_loss: 22.2933 - val_mean_absolute_error: 3.7402 - val_mean_absolute_percentage_error: 24.7760\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 37.0622 - mean_absolute_error: 4.6495 - mean_absolute_percentage_error: 21.5340 - val_loss: 18.6614 - val_mean_absolute_error: 3.4247 - val_mean_absolute_percentage_error: 22.8219\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 33.9375 - mean_absolute_error: 4.2653 - mean_absolute_percentage_error: 19.5950 - val_loss: 14.7443 - val_mean_absolute_error: 2.7806 - val_mean_absolute_percentage_error: 19.8396\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 37.5913 - mean_absolute_error: 4.3934 - mean_absolute_percentage_error: 18.9977 - val_loss: 17.1108 - val_mean_absolute_error: 3.1295 - val_mean_absolute_percentage_error: 22.8817\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 32.5041 - mean_absolute_error: 4.1735 - mean_absolute_percentage_error: 18.9556 - val_loss: 15.1243 - val_mean_absolute_error: 3.0063 - val_mean_absolute_percentage_error: 20.2639\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 31.1661 - mean_absolute_error: 3.9284 - mean_absolute_percentage_error: 17.6056 - val_loss: 21.8832 - val_mean_absolute_error: 3.7839 - val_mean_absolute_percentage_error: 23.9994\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 33.5656 - mean_absolute_error: 4.2283 - mean_absolute_percentage_error: 18.6504 - val_loss: 20.3637 - val_mean_absolute_error: 3.6155 - val_mean_absolute_percentage_error: 23.8895\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 38.5400 - mean_absolute_error: 4.4661 - mean_absolute_percentage_error: 20.1280 - val_loss: 21.7749 - val_mean_absolute_error: 3.6640 - val_mean_absolute_percentage_error: 25.9677\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 28.5176 - mean_absolute_error: 3.9061 - mean_absolute_percentage_error: 17.7037 - val_loss: 17.7141 - val_mean_absolute_error: 3.3379 - val_mean_absolute_percentage_error: 20.0170\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 36.4305 - mean_absolute_error: 4.3758 - mean_absolute_percentage_error: 19.1017 - val_loss: 16.1428 - val_mean_absolute_error: 2.9718 - val_mean_absolute_percentage_error: 21.1801\n",
      "Epoch 00095: early stopping\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:16\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'n_estimators': 2000, 'learning_rate': 0.15, 'num_leaves': 8, 'lambda_l2': 0.001, 'histogram_pool_size': 16384}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model LGBMRegressor to predict MEDV\n",
      "Started at:\n",
      "2019-02-10 23:14:59\n",
      "[1]\trandom_holdout_set_from_training_data's rmse: 7.81836\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\trandom_holdout_set_from_training_data's rmse: 7.02353\n",
      "[3]\trandom_holdout_set_from_training_data's rmse: 6.33738\n",
      "[4]\trandom_holdout_set_from_training_data's rmse: 5.89178\n",
      "[5]\trandom_holdout_set_from_training_data's rmse: 5.44069\n",
      "[6]\trandom_holdout_set_from_training_data's rmse: 5.09284\n",
      "[7]\trandom_holdout_set_from_training_data's rmse: 4.81442\n",
      "[8]\trandom_holdout_set_from_training_data's rmse: 4.60566\n",
      "[9]\trandom_holdout_set_from_training_data's rmse: 4.4407\n",
      "[10]\trandom_holdout_set_from_training_data's rmse: 4.3054\n",
      "[11]\trandom_holdout_set_from_training_data's rmse: 4.15802\n",
      "[12]\trandom_holdout_set_from_training_data's rmse: 4.09971\n",
      "[13]\trandom_holdout_set_from_training_data's rmse: 4.01898\n",
      "[14]\trandom_holdout_set_from_training_data's rmse: 3.93457\n",
      "[15]\trandom_holdout_set_from_training_data's rmse: 3.89352\n",
      "[16]\trandom_holdout_set_from_training_data's rmse: 3.86117\n",
      "[17]\trandom_holdout_set_from_training_data's rmse: 3.79059\n",
      "[18]\trandom_holdout_set_from_training_data's rmse: 3.78519\n",
      "[19]\trandom_holdout_set_from_training_data's rmse: 3.7424\n",
      "[20]\trandom_holdout_set_from_training_data's rmse: 3.70879\n",
      "[21]\trandom_holdout_set_from_training_data's rmse: 3.66042\n",
      "[22]\trandom_holdout_set_from_training_data's rmse: 3.64359\n",
      "[23]\trandom_holdout_set_from_training_data's rmse: 3.6616\n",
      "[24]\trandom_holdout_set_from_training_data's rmse: 3.65957\n",
      "[25]\trandom_holdout_set_from_training_data's rmse: 3.63168\n",
      "[26]\trandom_holdout_set_from_training_data's rmse: 3.62641\n",
      "[27]\trandom_holdout_set_from_training_data's rmse: 3.59232\n",
      "[28]\trandom_holdout_set_from_training_data's rmse: 3.60798\n",
      "[29]\trandom_holdout_set_from_training_data's rmse: 3.60524\n",
      "[30]\trandom_holdout_set_from_training_data's rmse: 3.54757\n",
      "[31]\trandom_holdout_set_from_training_data's rmse: 3.57301\n",
      "[32]\trandom_holdout_set_from_training_data's rmse: 3.52692\n",
      "[33]\trandom_holdout_set_from_training_data's rmse: 3.50541\n",
      "[34]\trandom_holdout_set_from_training_data's rmse: 3.49115\n",
      "[35]\trandom_holdout_set_from_training_data's rmse: 3.48922\n",
      "[36]\trandom_holdout_set_from_training_data's rmse: 3.50121\n",
      "[37]\trandom_holdout_set_from_training_data's rmse: 3.4959\n",
      "[38]\trandom_holdout_set_from_training_data's rmse: 3.474\n",
      "[39]\trandom_holdout_set_from_training_data's rmse: 3.43564\n",
      "[40]\trandom_holdout_set_from_training_data's rmse: 3.42919\n",
      "[41]\trandom_holdout_set_from_training_data's rmse: 3.42952\n",
      "[42]\trandom_holdout_set_from_training_data's rmse: 3.4156\n",
      "[43]\trandom_holdout_set_from_training_data's rmse: 3.42692\n",
      "[44]\trandom_holdout_set_from_training_data's rmse: 3.44753\n",
      "[45]\trandom_holdout_set_from_training_data's rmse: 3.44707\n",
      "[46]\trandom_holdout_set_from_training_data's rmse: 3.43627\n",
      "[47]\trandom_holdout_set_from_training_data's rmse: 3.45172\n",
      "[48]\trandom_holdout_set_from_training_data's rmse: 3.45409\n",
      "[49]\trandom_holdout_set_from_training_data's rmse: 3.45101\n",
      "[50]\trandom_holdout_set_from_training_data's rmse: 3.47392\n",
      "[51]\trandom_holdout_set_from_training_data's rmse: 3.49736\n",
      "[52]\trandom_holdout_set_from_training_data's rmse: 3.47912\n",
      "[53]\trandom_holdout_set_from_training_data's rmse: 3.49135\n",
      "[54]\trandom_holdout_set_from_training_data's rmse: 3.51304\n",
      "[55]\trandom_holdout_set_from_training_data's rmse: 3.49322\n",
      "[56]\trandom_holdout_set_from_training_data's rmse: 3.48587\n",
      "[57]\trandom_holdout_set_from_training_data's rmse: 3.48449\n",
      "[58]\trandom_holdout_set_from_training_data's rmse: 3.47085\n",
      "[59]\trandom_holdout_set_from_training_data's rmse: 3.48118\n",
      "[60]\trandom_holdout_set_from_training_data's rmse: 3.46743\n",
      "[61]\trandom_holdout_set_from_training_data's rmse: 3.47444\n",
      "[62]\trandom_holdout_set_from_training_data's rmse: 3.4761\n",
      "[63]\trandom_holdout_set_from_training_data's rmse: 3.46042\n",
      "[64]\trandom_holdout_set_from_training_data's rmse: 3.46895\n",
      "[65]\trandom_holdout_set_from_training_data's rmse: 3.47985\n",
      "[66]\trandom_holdout_set_from_training_data's rmse: 3.48235\n",
      "[67]\trandom_holdout_set_from_training_data's rmse: 3.46692\n",
      "[68]\trandom_holdout_set_from_training_data's rmse: 3.46449\n",
      "[69]\trandom_holdout_set_from_training_data's rmse: 3.4935\n",
      "[70]\trandom_holdout_set_from_training_data's rmse: 3.47535\n",
      "[71]\trandom_holdout_set_from_training_data's rmse: 3.46682\n",
      "[72]\trandom_holdout_set_from_training_data's rmse: 3.48848\n",
      "[73]\trandom_holdout_set_from_training_data's rmse: 3.46866\n",
      "[74]\trandom_holdout_set_from_training_data's rmse: 3.47528\n",
      "[75]\trandom_holdout_set_from_training_data's rmse: 3.45361\n",
      "[76]\trandom_holdout_set_from_training_data's rmse: 3.46413\n",
      "[77]\trandom_holdout_set_from_training_data's rmse: 3.45096\n",
      "[78]\trandom_holdout_set_from_training_data's rmse: 3.46075\n",
      "[79]\trandom_holdout_set_from_training_data's rmse: 3.47845\n",
      "[80]\trandom_holdout_set_from_training_data's rmse: 3.46623\n",
      "[81]\trandom_holdout_set_from_training_data's rmse: 3.47511\n",
      "[82]\trandom_holdout_set_from_training_data's rmse: 3.48807\n",
      "[83]\trandom_holdout_set_from_training_data's rmse: 3.46705\n",
      "[84]\trandom_holdout_set_from_training_data's rmse: 3.46682\n",
      "[85]\trandom_holdout_set_from_training_data's rmse: 3.46074\n",
      "[86]\trandom_holdout_set_from_training_data's rmse: 3.44476\n",
      "[87]\trandom_holdout_set_from_training_data's rmse: 3.45958\n",
      "[88]\trandom_holdout_set_from_training_data's rmse: 3.48004\n",
      "[89]\trandom_holdout_set_from_training_data's rmse: 3.48815\n",
      "[90]\trandom_holdout_set_from_training_data's rmse: 3.49792\n",
      "[91]\trandom_holdout_set_from_training_data's rmse: 3.49311\n",
      "[92]\trandom_holdout_set_from_training_data's rmse: 3.48297\n",
      "[93]\trandom_holdout_set_from_training_data's rmse: 3.49612\n",
      "[94]\trandom_holdout_set_from_training_data's rmse: 3.49192\n",
      "[95]\trandom_holdout_set_from_training_data's rmse: 3.47658\n",
      "[96]\trandom_holdout_set_from_training_data's rmse: 3.48145\n",
      "[97]\trandom_holdout_set_from_training_data's rmse: 3.48054\n",
      "[98]\trandom_holdout_set_from_training_data's rmse: 3.50068\n",
      "[99]\trandom_holdout_set_from_training_data's rmse: 3.49727\n",
      "[100]\trandom_holdout_set_from_training_data's rmse: 3.47498\n",
      "[101]\trandom_holdout_set_from_training_data's rmse: 3.4945\n",
      "[102]\trandom_holdout_set_from_training_data's rmse: 3.50981\n",
      "[103]\trandom_holdout_set_from_training_data's rmse: 3.51224\n",
      "[104]\trandom_holdout_set_from_training_data's rmse: 3.5105\n",
      "[105]\trandom_holdout_set_from_training_data's rmse: 3.50292\n",
      "[106]\trandom_holdout_set_from_training_data's rmse: 3.52044\n",
      "[107]\trandom_holdout_set_from_training_data's rmse: 3.51659\n",
      "[108]\trandom_holdout_set_from_training_data's rmse: 3.5042\n",
      "[109]\trandom_holdout_set_from_training_data's rmse: 3.49976\n",
      "[110]\trandom_holdout_set_from_training_data's rmse: 3.51528\n",
      "[111]\trandom_holdout_set_from_training_data's rmse: 3.53413\n",
      "[112]\trandom_holdout_set_from_training_data's rmse: 3.51226\n",
      "[113]\trandom_holdout_set_from_training_data's rmse: 3.52758\n",
      "[114]\trandom_holdout_set_from_training_data's rmse: 3.52293\n",
      "[115]\trandom_holdout_set_from_training_data's rmse: 3.53076\n",
      "[116]\trandom_holdout_set_from_training_data's rmse: 3.53473\n",
      "[117]\trandom_holdout_set_from_training_data's rmse: 3.51605\n",
      "[118]\trandom_holdout_set_from_training_data's rmse: 3.5247\n",
      "[119]\trandom_holdout_set_from_training_data's rmse: 3.51176\n",
      "[120]\trandom_holdout_set_from_training_data's rmse: 3.51346\n",
      "[121]\trandom_holdout_set_from_training_data's rmse: 3.51728\n",
      "[122]\trandom_holdout_set_from_training_data's rmse: 3.52425\n",
      "[123]\trandom_holdout_set_from_training_data's rmse: 3.53002\n",
      "[124]\trandom_holdout_set_from_training_data's rmse: 3.5144\n",
      "[125]\trandom_holdout_set_from_training_data's rmse: 3.52904\n",
      "[126]\trandom_holdout_set_from_training_data's rmse: 3.51477\n",
      "[127]\trandom_holdout_set_from_training_data's rmse: 3.52081\n",
      "[128]\trandom_holdout_set_from_training_data's rmse: 3.52261\n",
      "[129]\trandom_holdout_set_from_training_data's rmse: 3.52404\n",
      "[130]\trandom_holdout_set_from_training_data's rmse: 3.53591\n",
      "[131]\trandom_holdout_set_from_training_data's rmse: 3.53141\n",
      "[132]\trandom_holdout_set_from_training_data's rmse: 3.54353\n",
      "[133]\trandom_holdout_set_from_training_data's rmse: 3.54987\n",
      "[134]\trandom_holdout_set_from_training_data's rmse: 3.54089\n",
      "[135]\trandom_holdout_set_from_training_data's rmse: 3.54279\n",
      "[136]\trandom_holdout_set_from_training_data's rmse: 3.54691\n",
      "[137]\trandom_holdout_set_from_training_data's rmse: 3.54954\n",
      "[138]\trandom_holdout_set_from_training_data's rmse: 3.54592\n",
      "[139]\trandom_holdout_set_from_training_data's rmse: 3.54903\n",
      "[140]\trandom_holdout_set_from_training_data's rmse: 3.55256\n",
      "[141]\trandom_holdout_set_from_training_data's rmse: 3.53749\n",
      "[142]\trandom_holdout_set_from_training_data's rmse: 3.54058\n",
      "Early stopping, best iteration is:\n",
      "[42]\trandom_holdout_set_from_training_data's rmse: 3.4156\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:00\n",
      "\n",
      "\n",
      "Here are the results from our LGBMRegressor\n",
      "predicting MEDV\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name        |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 16 | feature_learning_3  |            0 |   0.6463 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 13 | CHAS=1.0            |            0 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 21 | feature_learning_8  |            0 |   0.6059 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 20 | feature_learning_7  |            0 |   0.2559 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 12 | CHAS=0.0            |            0 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 19 | feature_learning_6  |            0 |   0.2597 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 22 | feature_learning_9  |            0 |   0.7728 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 15 | feature_learning_2  |            0 |   0.5739 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 17 | feature_learning_4  |            1 |   0.5906 |            0.0104 |           -0.0407 |    0.0104 |    0.0407 |    0.0000 |    0.0000 |\n",
      "| 23 | feature_learning_10 |            1 |   0.2592 |           -0.0071 |            0.0081 |    0.0071 |    0.0081 |    0.0000 |    0.0000 |\n",
      "|  1 | ZN                  |            1 |  11.8852 |           -0.0037 |            0.0015 |    0.0037 |    0.0015 |    0.0000 |    0.0000 |\n",
      "|  7 | RAD                 |            3 |   0.1920 |           -0.0878 |            0.2233 |    0.0878 |    0.2233 |    0.0000 |    0.0000 |\n",
      "|  8 | TAX                 |            9 |   0.1818 |            0.3340 |           -0.1726 |    0.3358 |    0.1742 |    0.0000 |    0.0000 |\n",
      "|  2 | INDUS               |           13 |   0.1853 |            0.2808 |           -0.1218 |    0.3336 |    0.2923 |    0.0000 |    0.0000 |\n",
      "|  5 | AGE                 |           14 |   0.1635 |           -0.1029 |            0.0494 |    0.2438 |    0.2508 |    0.0520 |    0.0598 |\n",
      "|  9 | PTRATIO             |           15 |   0.1654 |            0.1088 |           -0.0714 |    0.2244 |    0.1730 |    0.0913 |    0.0281 |\n",
      "| 10 | B                   |           16 |   0.1373 |           -0.4080 |           -0.1425 |    0.5413 |    0.2509 |    0.5383 |    0.1147 |\n",
      "|  0 | CRIM                |           19 |   0.1517 |           -0.0130 |           -0.1656 |    0.2353 |    0.4205 |    0.0876 |    0.3505 |\n",
      "| 18 | feature_learning_5  |           21 |   0.2294 |            1.2560 |           -1.0219 |    1.3423 |    1.0838 |    0.4608 |    0.3279 |\n",
      "| 14 | feature_learning_1  |           25 |   0.6385 |           -0.2992 |            0.1484 |    0.4534 |    0.5303 |    0.1399 |    0.2940 |\n",
      "|  3 | NOX                 |           26 |   0.1472 |            0.2519 |           -0.2977 |    0.5386 |    0.6162 |    0.2094 |    0.2825 |\n",
      "|  4 | RM                  |           28 |   0.1321 |           -0.3903 |            0.5763 |    0.5228 |    0.7296 |    0.2041 |    0.2544 |\n",
      "|  6 | DIS                 |           36 |   0.1523 |            0.1979 |            0.2414 |    0.6583 |    0.7253 |    0.5141 |    0.5598 |\n",
      "| 11 | LSTAT               |           46 |   0.1431 |            1.7306 |           -1.3591 |    1.7779 |    1.4335 |    0.9735 |    0.6950 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "4.1914474180200205\n",
      "\n",
      "Here is the average of the predictions:\n",
      "21.442622187899122\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "21.488235294117654\n",
      "\n",
      "Here is the median prediction:\n",
      "21.034479644187087\n",
      "\n",
      "Here is the median actual value:\n",
      "20.15\n",
      "\n",
      "Here is the mean absolute error:\n",
      "2.500106761373544\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "1.7384825339524648\n",
      "\n",
      "Here is the explained variance:\n",
      "0.7604630479220672\n",
      "\n",
      "Here is the R-squared value:\n",
      "0.7604346768852921\n",
      "Count of positive differences (prediction > actual):\n",
      "55\n",
      "Count of negative differences:\n",
      "47\n",
      "Average positive difference:\n",
      "2.2759850256892036\n",
      "Average negative difference:\n",
      "-2.762376877599899\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed(random_state)\n",
    "\n",
    "# column_descriptions = {\n",
    "#     'targets': 'output'\n",
    "# }\n",
    "\n",
    "column_descriptions = {\n",
    "    'MEDV': 'output',\n",
    "    'CHAS': 'categorical'\n",
    "}\n",
    "\n",
    "ml_predictor = Predictor(\n",
    "    type_of_estimator='regressor', \n",
    "    column_descriptions=column_descriptions\n",
    ")\n",
    "\n",
    "ml_predictor.train(\n",
    "    df_train, \n",
    "    model_names = 'LGBMRegressor',\n",
    "    cv = kf,\n",
    "    feature_learning = True,\n",
    "#     fl_data = df_train.copy()\n",
    "    fl_data = fl_data\n",
    ")\n",
    "\n",
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df_test, df_test.MEDV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.5682314580267"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(df_test.MEDV, ml_predictor.predict(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-35585cb152cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m-\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_score' is not defined"
     ]
    }
   ],
   "source": [
    "-test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
